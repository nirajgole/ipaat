{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Module 5 Assignment\n",
    "\n",
    "```\n",
    "Dataset Description:\n",
    "* dispatching_base_number: The base station ID\n",
    "* date: Date\n",
    "* active_vehicles: The number of active vehicles\n",
    "* trips: Trips\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks to be Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring spark\n",
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://N7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>UberTripsDataAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27cb15c5150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('UberTripsDataAnalysis').master('local[2]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\n",
    "schema = StructType([\n",
    "    StructField(\"dispatching_base_number\", StringType(), False),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"active_vehicles\", IntegerType(), True),\n",
    "    StructField(\"trips\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+---------------+-----+\n",
      "|dispatching_base_number|      date|active_vehicles|trips|\n",
      "+-----------------------+----------+---------------+-----+\n",
      "|                 B02512|2015-01-01|            190| 1132|\n",
      "|                 B02765|2015-01-01|            225| 1765|\n",
      "|                 B02764|2015-01-01|           3427|29421|\n",
      "|                 B02682|2015-01-01|            945| 7679|\n",
      "|                 B02617|2015-01-01|           1228| 9537|\n",
      "|                 B02598|2015-01-01|            870| 6903|\n",
      "|                 B02598|2015-01-02|            785| 4768|\n",
      "|                 B02617|2015-01-02|           1137| 7065|\n",
      "|                 B02512|2015-01-02|            175|  875|\n",
      "|                 B02682|2015-01-02|            890| 5506|\n",
      "+-----------------------+----------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the dataset\n",
    "df=spark\\\n",
    "    .read\\\n",
    "    .format('csv')\\\n",
    "    .option('header',True)\\\n",
    "    .option('dateFormat','M/d/y')\\\n",
    "    .option('mode','dropmalformed')\\\n",
    "    .option('badRecordsPath','./badRecords')\\\n",
    "    .schema(schema)\\\n",
    "    .load('../data/Mod5_uber_data.csv')\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+---------------+-----+\n",
      "|dispatching_base_number|      date|active_vehicles|trips|\n",
      "+-----------------------+----------+---------------+-----+\n",
      "|                 B02512|2015-01-01|            190| 1132|\n",
      "|                 B02765|2015-01-01|            225| 1765|\n",
      "|                 B02764|2015-01-01|           3427|29421|\n",
      "|                 B02682|2015-01-01|            945| 7679|\n",
      "|                 B02617|2015-01-01|           1228| 9537|\n",
      "|                 B02598|2015-01-01|            870| 6903|\n",
      "|                 B02598|2015-01-02|            785| 4768|\n",
      "|                 B02617|2015-01-02|           1137| 7065|\n",
      "|                 B02512|2015-01-02|            175|  875|\n",
      "|                 B02682|2015-01-02|            890| 5506|\n",
      "+-----------------------+----------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a temporary SQL table of the dataset\n",
    "df.createOrReplaceTempView(\"UberTrip\")\n",
    "spark.sql(\"CREATE TEMPORARY VIEW UberTripTemp AS SELECT * FROM UberTrip\")\n",
    "rdd = spark.sql(\"SELECT * FROM UberTripTemp\")\n",
    "rdd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- active_vehicles: integer (nullable = true)\n",
      " |-- trips: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Print the schema of the table\n",
    "rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|unique_dispatch_number|\n",
      "+----------------------+\n",
      "|                B02512|\n",
      "|                B02598|\n",
      "|                B02682|\n",
      "|                B02765|\n",
      "|                B02617|\n",
      "|                B02764|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Print all the distinct ‘dispatching_base_number’\n",
    "distinct_dispatching_base_number=spark.sql('SELECT DISTINCT dispatching_base_number as unique_dispatch_number FROM UberTripTemp')\n",
    "distinct_dispatching_base_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Busiest base by trips: B02764'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Determine which dispatching base is the busiest based on the number of trips\n",
    "busiest_base=spark.sql('select dispatching_base_number \\\n",
    "                            from UberTripTemp\\\n",
    "                            group by dispatching_base_number\\\n",
    "                            order by sum(trips) desc\\\n",
    "                            limit 1').collect()[0][0]\n",
    "f'Busiest base by trips: {busiest_base}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+\n",
      "|dispatching_base_number|TotalTrips|\n",
      "+-----------------------+----------+\n",
      "|                 B02764|   1914449|\n",
      "+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Alternate method - Column Based Expression\n",
    "import pyspark.sql.functions as f\n",
    "rdd\\\n",
    "    .groupBy('dispatching_base_number')\\\n",
    "    .agg(f.sum('trips')\\\n",
    "    .alias('TotalTrips'))\\\n",
    "    .orderBy(f.desc('TotalTrips'))\\\n",
    "    .limit(1)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|TotalTrips|\n",
      "+----------+----------+\n",
      "|2015-02-20|    100915|\n",
      "|2015-02-14|    100345|\n",
      "|2015-02-21|     98380|\n",
      "|2015-02-13|     98024|\n",
      "|2015-01-31|     92257|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Determine the five busiest days based on the number of trips in the time range of the data\n",
    "spark.sql('select date, sum(trips) as TotalTrips\\\n",
    "            from UberTripTemp\\\n",
    "            group by date\\\n",
    "            order by sum(trips) desc\\\n",
    "            limit 5')\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|      date|TotalTrips|drank|\n",
      "+----------+----------+-----+\n",
      "|2015-02-20|    100915|    1|\n",
      "|2015-02-14|    100345|    2|\n",
      "|2015-02-21|     98380|    3|\n",
      "|2015-02-13|     98024|    4|\n",
      "|2015-01-31|     92257|    5|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate method - Window-Rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window=Window.orderBy(f.desc('TotalTrips'))\n",
    "rdd\\\n",
    "    .groupBy('date')\\\n",
    "    .agg(f.sum('trips').alias('TotalTrips'))\\\n",
    "    .withColumn('drank',f.dense_rank().over(window))\\\n",
    "    .where(f.col('drank')<=5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|average_active_vehicles_B02512|\n",
      "+------------------------------+\n",
      "|                           223|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Calculate the average number of active vehicles on the base station ‘B02512’\n",
    "average_no_of_active_vehicles=spark.sql('SELECT CEIL(AVG(active_vehicles)) as average_active_vehicles_B02512\\\n",
    "                                            FROM UberTripTemp\\\n",
    "                                            WHERE dispatching_base_number=\"B02512\"')\n",
    "average_no_of_active_vehicles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
