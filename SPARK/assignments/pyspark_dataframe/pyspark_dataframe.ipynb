{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\spark\\\\spark-3.4.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://N7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MoviesRating</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b171b25790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"MoviesRating\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.sparkContext.textFile(\"./movie_data/ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"]\n",
    "schema = StructType([StructField(field, StringType(), True) for field in fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_rdd = ratings.map(lambda x: x.split(\"::\")).map(\n",
    "    lambda x: (x[0], x[1], x[2], x[3])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserID|MovieID|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|   1193|     5|978300760|\n",
      "|     1|    661|     3|978302109|\n",
      "|     1|    914|     3|978301968|\n",
      "|     1|   3408|     4|978300275|\n",
      "|     1|   2355|     5|978824291|\n",
      "|     1|   1197|     3|978302268|\n",
      "|     1|   1287|     5|978302039|\n",
      "|     1|   2804|     5|978300719|\n",
      "|     1|    594|     4|978302268|\n",
      "|     1|    919|     4|978301368|\n",
      "|     1|    595|     5|978824268|\n",
      "|     1|    938|     4|978301752|\n",
      "|     1|   2398|     4|978302281|\n",
      "|     1|   2918|     4|978302124|\n",
      "|     1|   1035|     5|978301753|\n",
      "|     1|   2791|     4|978302188|\n",
      "|     1|   2687|     3|978824268|\n",
      "|     1|   2018|     4|978301777|\n",
      "|     1|   3105|     5|978301713|\n",
      "|     1|   2797|     4|978302039|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df = spark.createDataFrame(data=ratings_rdd, schema=schema)\n",
    "ratings_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersRdd = spark.sparkContext.textFile(\"./movie_data/users.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"]\n",
    "schema = StructType([StructField(field, StringType(), True) for field in fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----------+--------+\n",
      "|UserID|Gender|Age|Occupation|Zip-code|\n",
      "+------+------+---+----------+--------+\n",
      "|     1|     F|  1|        10|   48067|\n",
      "|     2|     M| 56|        16|   70072|\n",
      "|     3|     M| 25|        15|   55117|\n",
      "|     4|     M| 45|         7|   02460|\n",
      "|     5|     M| 25|        20|   55455|\n",
      "|     6|     F| 50|         9|   55117|\n",
      "|     7|     M| 35|         1|   06810|\n",
      "|     8|     M| 25|        12|   11413|\n",
      "|     9|     M| 25|        17|   61614|\n",
      "|    10|     F| 35|         1|   95370|\n",
      "|    11|     F| 25|         1|   04093|\n",
      "|    12|     M| 25|        12|   32793|\n",
      "|    13|     M| 45|         1|   93304|\n",
      "|    14|     M| 35|         0|   60126|\n",
      "|    15|     M| 25|         7|   22903|\n",
      "|    16|     F| 35|         0|   20670|\n",
      "|    17|     M| 50|         1|   95350|\n",
      "|    18|     F| 18|         3|   95825|\n",
      "|    19|     M|  1|        10|   48073|\n",
      "|    20|     M| 25|        14|   55113|\n",
      "+------+------+---+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "row_rdd = usersRdd.map(lambda x: x.split(\"::\")).map(\n",
    "    lambda x: (x[0], x[1], x[2], x[3], x[4])\n",
    ")\n",
    "users = spark.createDataFrame(data=row_rdd, schema=schema)\n",
    "users.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|value                                                   |\n",
      "+--------------------------------------------------------+\n",
      "|1::Toy Story (1995)::Animation|Children's|Comedy        |\n",
      "|2::Jumanji (1995)::Adventure|Children's|Fantasy         |\n",
      "|3::Grumpier Old Men (1995)::Comedy|Romance              |\n",
      "|4::Waiting to Exhale (1995)::Comedy|Drama               |\n",
      "|5::Father of the Bride Part II (1995)::Comedy           |\n",
      "|6::Heat (1995)::Action|Crime|Thriller                   |\n",
      "|7::Sabrina (1995)::Comedy|Romance                       |\n",
      "|8::Tom and Huck (1995)::Adventure|Children's            |\n",
      "|9::Sudden Death (1995)::Action                          |\n",
      "|10::GoldenEye (1995)::Action|Adventure|Thriller         |\n",
      "|11::American President, The (1995)::Comedy|Drama|Romance|\n",
      "|12::Dracula: Dead and Loving It (1995)::Comedy|Horror   |\n",
      "|13::Balto (1995)::Animation|Children's                  |\n",
      "|14::Nixon (1995)::Drama                                 |\n",
      "|15::Cutthroat Island (1995)::Action|Adventure|Romance   |\n",
      "|16::Casino (1995)::Drama|Thriller                       |\n",
      "|17::Sense and Sensibility (1995)::Drama|Romance         |\n",
      "|18::Four Rooms (1995)::Thriller                         |\n",
      "|19::Ace Ventura: When Nature Calls (1995)::Comedy       |\n",
      "|20::Money Train (1995)::Action                          |\n",
      "+--------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset as dataframe\n",
    "\n",
    "movies = spark.read.text(\"./movie_data/movies.dat\")\n",
    "movies.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the columns using '::'\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "fields = movies.withColumn(\"split_data\", f.split(f.col(\"value\"), \"::\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the MovieID, Title, and Genres (Year) columns\n",
    "extracted_df = fields.select(\n",
    "    f.col(\"split_data\")[0].alias(\"MovieID\"),\n",
    "    f.col(\"split_data\")[1].alias(\"Title\"),\n",
    "    f.col(\"split_data\")[2].alias(\"Genres\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split() missing 1 required positional argument: 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m years_and_genres_df \u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     extracted_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32mC:\\Program Files\\spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:160\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'pattern'"
     ]
    }
   ],
   "source": [
    "years_and_genres_df =(\n",
    "    extracted_df.withColumn('Year', f.split(f.col('Title')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
