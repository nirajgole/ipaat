{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f056b1e9-bc41-4393-8412-4a3078bf79a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### JDBC connection string\n",
    "\n",
    "Integrated Security (Windows authentication i.e. without username and password) and download JDBC driver for reference [click here](https://learn.microsoft.com/en-us/sql/connect/jdbc/connecting-with-ssl-encryption?view=sql-server-ver16)\n",
    "\n",
    "using java version - `JDK 20`\\\n",
    "using os version - `windows11`\\\n",
    "Put `mssql-jdbc_auth-12.4.1.x64.dll` in `jdk/bin/`\\\n",
    "If needs put downloaded dll, jar and jre in `spark/jars`\\\n",
    "If needs enable TCP/IP for SQL server -> search on windows for `mmc` i.e. SQL server configuration manager -> add snap in for SQL server Network Configuration -> Protocols for MSSQL-> enable TCP/IP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd4e337f-85c8-43f4-a067-8a5a9af8e0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6cb54e7-4bf8-4c20-b089-3d326f7c01de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Spark - SQL Server Integrated Authentication Example') \\\n",
    "    .master('local') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867832b9-343a-4f4f-b659-0ced6649307c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME='LiveClassAssignment'\n",
    "PORT='63739'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd992834-d58b-4435-b1b1-9d87e93a1a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connectionUrl = f'''\n",
    "jdbc:sqlserver://localhost:{PORT};\n",
    "databaseName={DATABASE_NAME};\n",
    "integratedSecurity=true;\n",
    "encrypt=true;\n",
    "trustServerCertificate=true;\n",
    "'''\n",
    "\n",
    "connectionUrl = \"\".join(line.strip() for line in connectionUrl.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117bbac7-34e9-42af-adcf-b848679ce9cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```sql\n",
    "SELECT Department,AVG(Salary) as gross_average_salary FROM EmployeeDetails e\n",
    "LEFT JOIN EmployeeBonus eb on e.Employee_id=eb.Employee_ref_id_FK\n",
    "GROUP BY Department\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6273f7d-7538-43f0-9689-daf5669041a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_db_table_data(tableName:str):\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", connectionUrl) \\\n",
    "    .option(\"dbtable\", tableName) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4f3b0e-c160-4944-9e82-0b153a0c81ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_details_df =get_db_table_data('EmployeeDetails')\n",
    "emp_bonus_df=get_db_table_data('EmployeeBonus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e0c15a-d51e-4fd6-acfe-238a1af0bf42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,count,max,min,monotonically_increasing_id\n",
    "\n",
    "# mssql_df.groupBy('Department')\\\n",
    "#     .agg(avg('salary').alias('average_salary'))\\\n",
    "#     .show(truncate=False)\n",
    "\n",
    "salary_by_department = emp_details_df.join(emp_bonus_df,emp_details_df['Employee_id']==emp_bonus_df['Employee_ref_id_FK'],'left')\\\n",
    "    .groupBy('Department')\\\n",
    "    .agg(avg('salary').alias('average_salary'),\n",
    "         count(\"*\").alias(\"no_of_employees\"),\n",
    "         max('salary').alias('maximum_salary'),\n",
    "         min('salary').alias('minimum_salary'),\n",
    "         )\\\n",
    "             .withColumn(\"id\", monotonically_increasing_id())\n",
    "    # .show(truncate=False)\n",
    "salary_by_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea5af2f0-4c81-415d-8eea-2e3fe0b8426a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: need to fix this to write data to table\n",
    "def load_df_to_db(df):\n",
    "    mode='overwrite'\n",
    "    # properties={\"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "    tableName='departmental_salary'\n",
    "    df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(mode)\\\n",
    "    .option(\"url\", connectionUrl) \\\n",
    "    .option(\"dbtable\", tableName) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca42b4a-a6b9-4d78-b601-c8744236f68a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_df_to_db(salary_by_department)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1db7416c-48cf-4a51-9d3a-35fffdaddd98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "spark_db_connection",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
