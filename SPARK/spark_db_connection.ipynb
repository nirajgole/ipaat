{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC connection string\n",
    "\n",
    "Integrated Security (Windows authentication i.e. without username and password) and download JDBC driver for reference [click here](https://learn.microsoft.com/en-us/sql/connect/jdbc/connecting-with-ssl-encryption?view=sql-server-ver16)\n",
    "\n",
    "using java version - `JDK 20`\\\n",
    "using os version - `windows11`\\\n",
    "Put `mssql-jdbc_auth-12.4.1.x64.dll` in `jdk/bin/`\\\n",
    "If needs put downloaded dll, jar and jre in `spark/jars`\\\n",
    "If needs enable TCP/IP for SQL server -> search on windows for `mmc` i.e. SQL server configuration manager -> add snap in for SQL server Network Configuration -> Protocols for MSSQL-> enable TCP/IP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\spark\\\\spark-3.4.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Spark - SQL Server Integrated Authentication Example') \\\n",
    "    .master('local') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME='LiveClassAssignment'\n",
    "PORT='63739'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectionUrl = f'''\n",
    "jdbc:sqlserver://localhost:{PORT};\n",
    "databaseName={DATABASE_NAME};\n",
    "integratedSecurity=true;\n",
    "encrypt=true;\n",
    "trustServerCertificate=true;\n",
    "'''\n",
    "\n",
    "connectionUrl = \"\".join(line.strip() for line in connectionUrl.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT Department,AVG(Salary) as gross_average_salary FROM EmployeeDetails e\n",
    "LEFT JOIN EmployeeBonus eb on e.Employee_id=eb.Employee_ref_id_FK\n",
    "GROUP BY Department\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_table_data(tableName:str):\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", connectionUrl) \\\n",
    "    .option(\"dbtable\", tableName) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_details_df =get_db_table_data('EmployeeDetails')\n",
    "emp_bonus_df=get_db_table_data('EmployeeBonus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------------+--------------+--------------+---+\n",
      "|Department|    average_salary|no_of_employees|maximum_salary|minimum_salary| id|\n",
      "+----------+------------------+---------------+--------------+--------------+---+\n",
      "|        HR|166666.66666666666|              3|        300000|        100000|  0|\n",
      "|     Admin|          250000.0|              5|        500000|         80000|  1|\n",
      "|   Account|          137500.0|              2|        200000|         75000|  2|\n",
      "+----------+------------------+---------------+--------------+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,count,max,min,monotonically_increasing_id\n",
    "\n",
    "# mssql_df.groupBy('Department')\\\n",
    "#     .agg(avg('salary').alias('average_salary'))\\\n",
    "#     .show(truncate=False)\n",
    "\n",
    "salary_by_department = emp_details_df.join(emp_bonus_df,emp_details_df['Employee_id']==emp_bonus_df['Employee_ref_id_FK'],'left')\\\n",
    "    .groupBy('Department')\\\n",
    "    .agg(avg('salary').alias('average_salary'),\n",
    "         count(\"*\").alias(\"no_of_employees\"),\n",
    "         max('salary').alias('maximum_salary'),\n",
    "         min('salary').alias('minimum_salary'),\n",
    "         )\\\n",
    "             .withColumn(\"id\", monotonically_increasing_id())\n",
    "    # .show(truncate=False)\n",
    "salary_by_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: need to fix this to write data to table\n",
    "def load_df_to_db(df):\n",
    "    mode='overwrite'\n",
    "    # properties={\"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "    tableName='departmental_salary'\n",
    "    df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(mode)\\\n",
    "    .option(\"url\", connectionUrl) \\\n",
    "    .option(\"dbtable\", tableName) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'withColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Gole_Niraj\\IPaat\\ipaat\\SPARK\\spark_db_connection.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m load_df_to_db(salary_by_department)\n",
      "\u001b[1;32me:\\Gole_Niraj\\IPaat\\ipaat\\SPARK\\spark_db_connection.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# properties={\"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tableName\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdepartmental_salary\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, monotonically_increasing_id())\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mjdbc\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m.\u001b[39mmode(mode)\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, connectionUrl) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdbtable\u001b[39m\u001b[39m\"\u001b[39m, tableName) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdriver\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Gole_Niraj/IPaat/ipaat/SPARK/spark_db_connection.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m.\u001b[39msave()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'withColumn'"
     ]
    }
   ],
   "source": [
    "load_df_to_db(salary_by_department)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
